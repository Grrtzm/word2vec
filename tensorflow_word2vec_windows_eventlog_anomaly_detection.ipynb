{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503d0a3d",
   "metadata": {
    "id": "503d0a3d"
   },
   "source": [
    "<table style=\"border:1px solid black;border-collapse:collapse;\" align=\"left\">\n",
    "  <td style=\"border:1px solid red;\">\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/Grrtzm/word2vec/blob/main/tensorflow_word2vec_windows_eventlog_anomaly_detection.ipynb\">Run this notebook in Google Colab</a>\n",
    "  </td>\n",
    "  <td style=\"border:1px solid red;\">\n",
    "    <a target=\"_blank\" href=\"https://github.com/Grrtzm/word2vec/blob/main/tensorflow_word2vec_windows_eventlog_anomaly_detection.ipynb\">View source on GitHub</a>\n",
    "  </td>\n",
    "  <td style=\"border:1px solid red;\">\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/word2vec\">View original Word2Vec tutorial on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td style=\"border:1px solid red;\">\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/word2vec.ipynb\">Download original Tensorflow Word2Vec tutorial notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20254d2c",
   "metadata": {
    "id": "20254d2c"
   },
   "source": [
    "# Tensorflow Word2Vec on Windows Eventlogs\n",
    "This notebook is part of a project that uses Word2Vec for anomaly detection in Windows 10 event logs.<br>\n",
    "It uses a dataset which consists of all events derived from the System event log from my own PC.<br>\n",
    "The data was read using Powershell and Get-WinEvent. You can find the Powershell script and the python script for parsing in my github repository.<br>\n",
    "This version uses the Tensorflow implementation of Word2Vec, <a target=\"_blank\" href=\"https://colab.research.google.com/github/Grrtzm/word2vec/blob/main/gensim_word2vec_windows_eventlog_anomaly_detection.ipynb\">this version uses a Gensim implementation of Word2Vec</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada241e",
   "metadata": {
    "id": "0ada241e"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffece1a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11341,
     "status": "ok",
     "timestamp": 1641216498310,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "ffece1a4",
    "outputId": "d4332980-13fa-4d9e-ce4f-0f95597d591f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "!pip install -q tensorflow==2.5.0\n",
    "if tf.__version__!='2.5.0': wait = input(\"Tensorflow 2.5.0 was installed. Please restart the runtime and re-run the cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f588c9",
   "metadata": {
    "executionInfo": {
     "elapsed": 2509,
     "status": "ok",
     "timestamp": 1641216500962,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "87f588c9"
   },
   "outputs": [],
   "source": [
    "from os.path import exists as file_exists\n",
    "# Uncomment the type of event log file you would like to use.\n",
    "# When you want to download a new file, change 'False' to 'True' in the 'if' line.\n",
    "if file_exists('dataset.csv') == False:\n",
    "    # !pip install gdown      # uncomment if gdown is missing\n",
    "    import gdown\n",
    "    url = 'https://drive.google.com/uc?id=1Kt1FsUwVVTRkpxt7urykgOgDbeOpYRVn' # System events, original dataset, 12-3-2021 to 2-12-2021\n",
    "    # url = 'https://drive.google.com/uc?id=1-pmm8IR8ninf_ArRgG2rv8P0Lmi4w04b' # System events, dataset 12-3-2021 to 10-1-2022\n",
    "    # url = 'https://drive.google.com/uc?id=1cLb2GArKPStaggTH5RtYt6aOgCiEif38' # Application events, dataset 12-3-2021 to 10-1-2022\n",
    "    # url = 'https://drive.google.com/uc?id=1jq6LVEJbOqedzbZYS14njz_N5PKE-aXc' # NTFS operational events, dataset 12-3-2021 to 10-1-2022\n",
    "    # I didn't supply a Security event log, it's size is 143MB and it only contains 30 different Event ID's.\n",
    "    output = 'dataset.csv'\n",
    "    gdown.download(url, output, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23c87c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1641216501536,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "b23c87c5",
    "outputId": "72ce241b-21cf-4366-c03e-be8127312749"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeCreated</th>\n",
       "      <th>EventRecordID</th>\n",
       "      <th>EventID</th>\n",
       "      <th>Level</th>\n",
       "      <th>Provider</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-02 20:05:22.768441+00:00</td>\n",
       "      <td>37593</td>\n",
       "      <td>System_1014</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Microsoft-Windows-DNS-Client</td>\n",
       "      <td>Name resolution for the name config.teams.micr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-02 20:05:22.255612+00:00</td>\n",
       "      <td>37592</td>\n",
       "      <td>System_32</td>\n",
       "      <td>Information</td>\n",
       "      <td>e1dexpress</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-02 20:05:19.153883+00:00</td>\n",
       "      <td>37591</td>\n",
       "      <td>System_6062</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Netwtw08</td>\n",
       "      <td>6062 - Lso was triggered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-02 20:05:18.362357+00:00</td>\n",
       "      <td>37590</td>\n",
       "      <td>System_27</td>\n",
       "      <td>Warning</td>\n",
       "      <td>e1dexpress</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-02 20:05:18.278408+00:00</td>\n",
       "      <td>37589</td>\n",
       "      <td>System_32</td>\n",
       "      <td>Information</td>\n",
       "      <td>e1dexpress</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TimeCreated  EventRecordID      EventID        Level  \\\n",
       "0 2021-12-02 20:05:22.768441+00:00          37593  System_1014      Warning   \n",
       "1 2021-12-02 20:05:22.255612+00:00          37592    System_32  Information   \n",
       "2 2021-12-02 20:05:19.153883+00:00          37591  System_6062      Warning   \n",
       "3 2021-12-02 20:05:18.362357+00:00          37590    System_27      Warning   \n",
       "4 2021-12-02 20:05:18.278408+00:00          37589    System_32  Information   \n",
       "\n",
       "                       Provider  \\\n",
       "0  Microsoft-Windows-DNS-Client   \n",
       "1                    e1dexpress   \n",
       "2                      Netwtw08   \n",
       "3                    e1dexpress   \n",
       "4                    e1dexpress   \n",
       "\n",
       "                                             Message  \n",
       "0  Name resolution for the name config.teams.micr...  \n",
       "1                                                NaN  \n",
       "2                           6062 - Lso was triggered  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dot, Embedding, Flatten\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "from time import time  # To time our operations\n",
    "# from collections import defaultdict  # For word frequency\n",
    "from datetime import datetime # Voor DateTime -> datum bewerkingen\n",
    "\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Set the number of negative samples per positive context.\n",
    "# Key point: num_ns (number of negative samples per positive context word) \n",
    "# between [5, 20] is shown to work best for smaller datasets, while num_ns between [2,5] suffices for larger datasets.\n",
    "num_ns = 2 # was 5\n",
    "window_size = 5\n",
    "embedding_dim = 20 #  20 seems to be sufficient, 128 is the default value from the tutorial   # Dimension of the dense embedding.\n",
    "vocab_size = 20000 # inital size of the vocabulary. We will resize it later before defining the model.\n",
    "sequence_length = 40 # Number of words in a sentence.\n",
    "epochs = 3000 # Number of training epochs for Word2Vec\n",
    "use_trained_model = False\n",
    "\n",
    "df = pd.read_csv('dataset.csv', parse_dates=[\"TimeCreated\"]) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617369b7",
   "metadata": {
    "id": "617369b7"
   },
   "source": [
    "## Create Event \"word\" from multiple columns\n",
    "This creates a new column containing the words Word2Vec will be trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "544bcd8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1641216501993,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "544bcd8a",
    "outputId": "3728fc7a-1f9a-46ee-d5a5-40d596cc0238"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeCreated</th>\n",
       "      <th>EventRecordID</th>\n",
       "      <th>Message</th>\n",
       "      <th>Event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-02 20:05:22.768441+00:00</td>\n",
       "      <td>37593</td>\n",
       "      <td>Name resolution for the name config.teams.micr...</td>\n",
       "      <td>system1014warningmicrosoftwindowsdnsclient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-02 20:05:22.255612+00:00</td>\n",
       "      <td>37592</td>\n",
       "      <td>NaN</td>\n",
       "      <td>system32informatione1dexpress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-02 20:05:19.153883+00:00</td>\n",
       "      <td>37591</td>\n",
       "      <td>6062 - Lso was triggered</td>\n",
       "      <td>system6062warningnetwtw08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-02 20:05:18.362357+00:00</td>\n",
       "      <td>37590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>system27warninge1dexpress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-02 20:05:18.278408+00:00</td>\n",
       "      <td>37589</td>\n",
       "      <td>NaN</td>\n",
       "      <td>system32informatione1dexpress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-12-02 20:05:17.638712+00:00</td>\n",
       "      <td>37588</td>\n",
       "      <td>The system has returned from a low power state...</td>\n",
       "      <td>system1informationmicrosoftwindowspowertrouble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-12-02 20:05:17.544184+00:00</td>\n",
       "      <td>37587</td>\n",
       "      <td>7021 - Connection telemetry fields and analysi...</td>\n",
       "      <td>system7021informationnetwtw08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-12-02 20:05:15.997321+00:00</td>\n",
       "      <td>37586</td>\n",
       "      <td>Windows cannot store Bluetooth authentication ...</td>\n",
       "      <td>system18informationbthusb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TimeCreated  EventRecordID  \\\n",
       "0 2021-12-02 20:05:22.768441+00:00          37593   \n",
       "1 2021-12-02 20:05:22.255612+00:00          37592   \n",
       "2 2021-12-02 20:05:19.153883+00:00          37591   \n",
       "3 2021-12-02 20:05:18.362357+00:00          37590   \n",
       "4 2021-12-02 20:05:18.278408+00:00          37589   \n",
       "5 2021-12-02 20:05:17.638712+00:00          37588   \n",
       "6 2021-12-02 20:05:17.544184+00:00          37587   \n",
       "7 2021-12-02 20:05:15.997321+00:00          37586   \n",
       "\n",
       "                                             Message  \\\n",
       "0  Name resolution for the name config.teams.micr...   \n",
       "1                                                NaN   \n",
       "2                           6062 - Lso was triggered   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5  The system has returned from a low power state...   \n",
       "6  7021 - Connection telemetry fields and analysi...   \n",
       "7  Windows cannot store Bluetooth authentication ...   \n",
       "\n",
       "                                               Event  \n",
       "0         system1014warningmicrosoftwindowsdnsclient  \n",
       "1                      system32informatione1dexpress  \n",
       "2                          system6062warningnetwtw08  \n",
       "3                          system27warninge1dexpress  \n",
       "4                      system32informatione1dexpress  \n",
       "5  system1informationmicrosoftwindowspowertrouble...  \n",
       "6                      system7021informationnetwtw08  \n",
       "7                          system18informationbthusb  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punct = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{}~'   # `|` is not present here\n",
    "transtab = str.maketrans(dict.fromkeys(punct, ''))\n",
    "\n",
    "# Define a function to remove spaces\n",
    "# https://iqcode.com/code/python/pandas-series-remove-punctuation\n",
    "# and https://stackoverflow.com/questions/50444346/fast-punctuation-removal-with-pandas\n",
    "def remove_spaces(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(' ', '')\n",
    "    return text\n",
    "\n",
    "# Create a new column. Concatenate the columns, remove unwanted characters and convert to lowercase\n",
    "df['Event'] = df['EventID'].map(str) + df['Level'].map(str) + df['Provider'].apply(remove_spaces).map(str)\n",
    "df['Event'] = '|'.join(df['Event'].tolist()).translate(transtab).split('|') # remove all other unwanted characters\n",
    "df['Event'] = df['Event'].str.lower()\n",
    "\n",
    "# Delete the redundant columns\n",
    "df = df.drop(['EventID','Level','Provider'], axis=1)\n",
    "\n",
    "# Show a preview\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e37ff406",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1641216501993,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "e37ff406",
    "outputId": "7a3c6b85-b764-44e7-d3e7-9e04531c8cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines/events in the dataset: 37593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_rows = len(df.axes[0])\n",
    "print(f\"Number of lines/events in the dataset: {num_rows}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088c13e",
   "metadata": {
    "id": "5088c13e"
   },
   "source": [
    "## Create csv file corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c1f2e",
   "metadata": {
    "id": "510c1f2e"
   },
   "source": [
    "Create the text dataset \"eventlist\", a list containing \"eventrow\" lists. An \"eventrow\" list contains all events for that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf73dba9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9274,
     "status": "ok",
     "timestamp": 1641216511260,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "bf73dba9",
    "outputId": "00f33a4b-00fa-4457-c152-38d657851f5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines = 263, shortest line = 2 words, longest line = 1011 words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "minlength = 1e6\n",
    "maxlength = 0\n",
    "eventlist = []\n",
    "eventrow = []\n",
    "previous_date = None # datetime.now().date()\n",
    "for idx, row in df.iterrows():\n",
    "    date = row['TimeCreated'].date()\n",
    "    eventrow.append(row['Event'])\n",
    "    if date != previous_date:\n",
    "        length = len(eventrow) + 1\n",
    "        if length > maxlength: maxlength = length\n",
    "        if length < minlength: minlength = length\n",
    "        eventrow = []\n",
    "        eventlist.append(eventrow)\n",
    "        previous_date = date\n",
    "        \n",
    "print(f\"Number of lines = {len(eventlist)}, shortest line = {minlength} words, longest line = {maxlength} words\\n\")\n",
    "\n",
    "# Ugly, but the easiest way. Save the list to a csv file and read that as a text file.\n",
    "# Save the dataset as a csv file using the numpy module\n",
    "events = np.array(eventlist, dtype=object)\n",
    "np.savetxt('eventlist.csv', events, delimiter=',', fmt ='% s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcba531a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1641216511260,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "fcba531a",
    "outputId": "5d2b6173-0956-4764-919c-b8245c45a09a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`sequence_length` is changed now from 40 to 1011\n"
     ]
    }
   ],
   "source": [
    "print(f\"`sequence_length` is changed now from {sequence_length} to {maxlength}\")\n",
    "sequence_length = maxlength"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6f3ff",
   "metadata": {
    "id": "5ae6f3ff"
   },
   "source": [
    "### Vectorize sentences from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073890e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1455,
     "status": "ok",
     "timestamp": 1641216512704,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "073890e1",
    "outputId": "ed1914db-86bd-4fce-f94c-8b385828b232"
   },
   "outputs": [],
   "source": [
    "path_to_file = \"eventlist.csv\"\n",
    "\n",
    "print(f\"File containing log events: {path_to_file}\\n\")\n",
    "\n",
    "# Use the non empty lines to construct a tf.data.TextLineDataset object for next steps.\n",
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "\n",
    "'''\n",
    "# Uncomment this block to see how the data (tensors) looks like.\n",
    "for line in text_ds.take(10):\n",
    "    print(line)\n",
    "print(f\"Those were the tensors\\n\")\n",
    "\n",
    "print(text_ds)\n",
    "print()\n",
    "'''\n",
    "\n",
    "# Now, create a custom standardization function to lowercase the text and remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    return tf.strings.regex_replace(lowercase,\n",
    "    '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Set output_sequence_length length to pad all samples to same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size, # None\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Call adapt on the text dataset to create vocabulary.\n",
    "vectorize_layer.adapt(text_ds.batch(1024))\n",
    "\n",
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(f\"Number of words in vocabulary (limit is {vocab_size}): {len(inverse_vocab)-1}\")\n",
    "\n",
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "\n",
    "# Obtain number of sequences from the dataset\n",
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "\n",
    "print(f\"First 10 sequences:\\n\")\n",
    "idx = 0\n",
    "for seq in sequences:\n",
    "    print(seq)\n",
    "    idx = idx + 1\n",
    "    if idx > 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c31452",
   "metadata": {
    "id": "c1c31452"
   },
   "source": [
    "## Show the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b12d47b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1641216513083,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "8b12d47b",
    "outputId": "673495cc-b534-4849-821b-00efc4790bc9"
   },
   "outputs": [],
   "source": [
    "vocab_len = len(inverse_vocab)\n",
    "print(f\"Number of words in vocabulary (limit is {vocab_size}): {vocab_len - 1}\\n\")\n",
    "v=0\n",
    "for i in range(vocab_len):\n",
    "    if i>0: \n",
    "        # I needed to shift the weight index in order to match the values from u with the inverse_vocab\n",
    "        # v = vectorize_layer.get_weights()[0][i-2]\n",
    "        v = vectorize_layer.get_weights()[1][i-2]  # in Tensorflow 2.5.0\n",
    "    w = inverse_vocab[i]\n",
    "    print(f\"Index {i}:\\tWeight:{v}\\t{w}\")\n",
    "    #if i > 5: break # only show the first 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e01c71",
   "metadata": {
    "id": "59e01c71"
   },
   "source": [
    "### Helper function to generate training data\n",
    "Generates skip-gram pairs with negative sampling for a list of sequences (int-encoded sentences) \n",
    "based on window size, number of negative samples and vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4e966",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1641216513084,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "a9f4e966"
   },
   "outputs": [],
   "source": [
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "    # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    # Build the sampling table for vocab_size tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    # Iterate over all sequences (sentences) in dataset.\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "        # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "              sequence,\n",
    "              vocabulary_size=vocab_size,\n",
    "              sampling_table=sampling_table,\n",
    "              window_size=window_size,\n",
    "              negative_samples=0)\n",
    "\n",
    "        # Iterate over each positive skip-gram pair to produce training examples\n",
    "        # with positive context word and negative samples.\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(\n",
    "                tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=SEED,\n",
    "                name=\"negative_sampling\")\n",
    "\n",
    "            # Build context and label vectors (for one target word)\n",
    "            negative_sampling_candidates = tf.expand_dims(\n",
    "                negative_sampling_candidates, 1)\n",
    "\n",
    "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dceb9d5",
   "metadata": {
    "id": "4dceb9d5"
   },
   "source": [
    "### Generate training examples from sequences\n",
    "`sequences` is now a list of int encoded sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b3f983",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8170,
     "status": "ok",
     "timestamp": 1641216521252,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "87b3f983",
    "outputId": "6451873c-e2c7-48ab-d460-544eff3ce4ee"
   },
   "outputs": [],
   "source": [
    "print(f\"Genererate training examples from sequences, using: window_size={window_size}, num_ns={num_ns}, vocab_size={vocab_len}, seed={SEED}\")\n",
    "import time\n",
    "time.sleep(1) # Nodig om tekst op de juiste plaats terecht te laten komen (i.v.m. multi-threading)\n",
    "targets, contexts, labels = generate_training_data(\n",
    "    # sequences=sequences,\n",
    "    sequences=sequences,\n",
    "    window_size=window_size,\n",
    "    num_ns=num_ns,\n",
    "    vocab_size=vocab_len,\n",
    "    seed=SEED)\n",
    "time.sleep(1) # Nodig om tekst op de juiste plaats terecht te laten komen (i.v.m. multi-threading)\n",
    "print(f\"Number of training examples; targets:{len(targets)}, contexts:{len(contexts)}, labels:{len(labels)}\")\n",
    "\n",
    "# Configure the dataset for performance\n",
    "BATCH_SIZE = len(targets)\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# Add cache() and prefetch() to improve performance.\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593af9a",
   "metadata": {
    "id": "3593af9a"
   },
   "source": [
    "### Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e8a88c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30850,
     "status": "ok",
     "timestamp": 1641216552090,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "29e8a88c",
    "outputId": "7c9ed5d3-a3a8-40b5-dc3f-bf286a37cc84"
   },
   "outputs": [],
   "source": [
    "class trainingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>0.9):\n",
    "            print(f\"\\nReached 90% accuracy in {epoch+1} epochs. Ended training...\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "class Word2Vec(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "        self.context_embedding = Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "        self.dots = Dot(axes=(3, 2))\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        word_emb = self.target_embedding(target)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        dots = self.dots([context_emb, word_emb])\n",
    "        \n",
    "        return self.flatten(dots)\n",
    "\n",
    "if use_trained_model == False:\n",
    "    # Define loss function and compile model\n",
    "    model = Word2Vec(vocab_len, embedding_dim)\n",
    "    model.compile(optimizer='adam',\n",
    "                     loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "    # Train the model with dataset prepared above for some number of epochs.\n",
    "    print(\"Training Word2Vec...\")\n",
    "    history = model.fit(dataset, epochs=epochs, verbose=0, callbacks=[trainingCallback()]) # verbose=0 means no output, verbose=1 shows logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e2fcf",
   "metadata": {
    "id": "e83e2fcf"
   },
   "source": [
    "## Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c721fed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 736,
     "status": "ok",
     "timestamp": 1641216552824,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "9c721fed",
    "outputId": "e5f67d43-76ad-4e78-abbb-1eb7f0542da7"
   },
   "outputs": [],
   "source": [
    "if use_trained_model == False:\n",
    "    model.save('tensorflow_word2vec_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83154da4",
   "metadata": {
    "id": "83154da4"
   },
   "source": [
    "### Show some training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e7328e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 595,
     "status": "ok",
     "timestamp": 1641216553415,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "98e7328e",
    "outputId": "cdac4a7b-104a-4c7c-cf71-58d810438a50"
   },
   "outputs": [],
   "source": [
    "if use_trained_model == False:\n",
    "\n",
    "    import matplotlib.image  as mpimg\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    acc=history.history['accuracy']\n",
    "    loss=history.history['loss']\n",
    "\n",
    "    epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "    plt.title(\"Tensorflow Word2Vec Training loss and accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.plot(epochs, acc, color='r', label='Accuracy')\n",
    "    plt.plot(epochs, loss, color='b', label='Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"tensorflow_word2vec_training_loss_accuracy.png\", format=\"png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d4bf7f",
   "metadata": {
    "id": "25d4bf7f"
   },
   "source": [
    "## Load trained model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c485e017",
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1641216553721,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "c485e017"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('tensorflow_word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4467778",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1641216553721,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "c4467778",
    "outputId": "147d5952-e7c6-47e4-fc3d-cc7a36c349eb"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e046fad",
   "metadata": {
    "id": "8e046fad"
   },
   "source": [
    "### Embedding lookup and analysis\n",
    "Generate the vectors.tsv and metadata.tsv to analyze the obtained embeddings in the Embedding Projector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18944aa4",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1641216553722,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "18944aa4"
   },
   "outputs": [],
   "source": [
    "weights = model.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "# Create and save the vectors and metadata file\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index < 1:\n",
    "        continue  # skip 0, it's padding\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb7397",
   "metadata": {
    "id": "2ceb7397"
   },
   "source": [
    "Run the cell below to download the `vectors.tsv` and `metadata.tsv` to analyze the obtained embeddings in the [Embedding Projector](https://projector.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ce806",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1641216553722,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "de6ce806",
    "outputId": "badd0064-8ea9-4f49-ff12-e2edbf90cac5"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('vectors.tsv')\n",
    "    files.download('metadata.tsv')\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e891e40",
   "metadata": {
    "id": "8e891e40"
   },
   "source": [
    "## Calculate `cosine_similarity` for each event\n",
    "And add these as a a column to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305be91",
   "metadata": {
    "executionInfo": {
     "elapsed": 6996,
     "status": "ok",
     "timestamp": 1641216560714,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "3305be91"
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def vocab_lookup(eventstr):\n",
    "    for index, word in enumerate(vocab):\n",
    "        if index < 1:\n",
    "            continue  # skip 0, it's padding\n",
    "        if word == eventstr:\n",
    "            vec = weights[index]\n",
    "            break\n",
    "    return vec\n",
    "\n",
    "num_events = len(df.axes[0])\n",
    "# add empty columns\n",
    "cos_sim = []\n",
    "for idx, row in df.iterrows():\n",
    "    current_event = row['Event']\n",
    "    ce = vocab_lookup(current_event)\n",
    "    if idx == 0:\n",
    "        cos_sim.append(float(1))\n",
    "        previous_event = current_event\n",
    "        pe = ce\n",
    "    if idx > 0:\n",
    "        if idx < num_events + 1:\n",
    "            cs = dot(pe, ce)/(norm(pe)*norm(ce)) # https://www.statology.org/cosine-similarity-python/\n",
    "            cos_sim.append(cs)\n",
    "            previous_event = current_event\n",
    "            pe = ce\n",
    "\n",
    "df['cos_sim'] = cos_sim\n",
    "\n",
    "# Change order of columns by name, so we can display it orderly\n",
    "df = df[['TimeCreated', 'EventRecordID', 'Event', 'cos_sim', 'Message']]\n",
    "df.head()\n",
    "\n",
    "# saving the dataframe\n",
    "df.to_csv('System-Events-similarity-Tensorflow.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212048fe",
   "metadata": {
    "id": "212048fe"
   },
   "source": [
    "## Plot all anomalies\n",
    "The lower the line spikes, the more unique it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735e308",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "executionInfo": {
     "elapsed": 6646,
     "status": "ok",
     "timestamp": 1641216567358,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "1735e308",
    "outputId": "58ba7c6d-8f72-43cc-8a95-c1825255f8ab"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "print(\"Please note the pan/zoom controls in the ModeBar on the right...\")\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "config = {'displayModeBar': True}\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(df.TimeCreated), y=list(df.cos_sim)))\n",
    "\n",
    "# Set title\n",
    "fig.update_layout(\n",
    "    title_text=\"Time series with range slider and selectors\"\n",
    ")\n",
    "\n",
    "# Add range slider\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1,\n",
    "                     label=\"1d\",\n",
    "                     step=\"day\",\n",
    "                     stepmode=\"todate\"),\n",
    "                dict(count=7,\n",
    "                     label=\"7d\",\n",
    "                     step=\"day\",\n",
    "                     stepmode=\"todate\"),\n",
    "                dict(count=1,\n",
    "                     label=\"1m\",\n",
    "                     step=\"month\",\n",
    "                     stepmode=\"todate\"),\n",
    "                dict(step=\"all\")\n",
    "            ])\n",
    "        ),\n",
    "\n",
    "        rangeslider=dict(\n",
    "            visible=True\n",
    "        ),\n",
    "        type=\"date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# fig.update_traces(mode='markers+lines') # displays dots (markers) on each event. Very slow!\n",
    "fig.show(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f9f055",
   "metadata": {
    "id": "f8f9f055"
   },
   "source": [
    "## Show Top 10 of all 'anomalies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a67dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1641216567359,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "861a67dc",
    "outputId": "e1947bdc-e85a-45d8-986f-0e0a0779a458"
   },
   "outputs": [],
   "source": [
    "# Pandas display settings\n",
    "pd.set_option('display.max_columns', None) # Set it to None to display all columns in the dataframe\n",
    "pd.set_option('display.width',200)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"Top 10 of all 'anomalies':\\n\")\n",
    "df.nsmallest(n=10, columns=['cos_sim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b5b86e",
   "metadata": {
    "id": "e3b5b86e"
   },
   "source": [
    "As you can see above, those anomalies are not always not what you expected. We are not interested in 'Information' events. \n",
    "<br> Now lets take a look at some more interesting events.\n",
    "## Show Top 10 of 'critical' anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0b596",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1641216567359,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "84c0b596",
    "outputId": "6727baef-3327-4f97-c18e-13cdd20c9684"
   },
   "outputs": [],
   "source": [
    "dfcritical = df[df['Event'].str.contains('critical')]\n",
    "print(f\"Top 10 of 'critical' anomalies from a total of {len(dfcritical)} 'critical' events:\\n\")\n",
    "dfcritical.nsmallest(n=10, columns=['cos_sim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6271295e",
   "metadata": {
    "id": "6271295e"
   },
   "source": [
    "## Show Top 10 of 'error' anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dacfcaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1641216567360,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "7dacfcaf",
    "outputId": "187679c1-5737-4da2-8788-29cb1082f820"
   },
   "outputs": [],
   "source": [
    "dferror = df[df['Event'].str.contains('error')]\n",
    "print(f\"Top 10 of 'error' anomalies from a total of {len(dferror)} 'error' events:\\n\")\n",
    "dferror.nsmallest(n=10, columns=['cos_sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c650b",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1641216567360,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "104c650b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Kopie van Kopie van windows_eventlog_anomaly_detection_with_tensorflow_word2vec.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/Grrtzm/word2vec/blob/main/windows_eventlog_anomaly_detection_with_tensorflow_word2vec.ipynb",
     "timestamp": 1641216650875
    },
    {
     "file_id": "https://github.com/Grrtzm/word2vec/blob/main/windows_eventlog_anomaly_detection_with_tensorflow_word2vec.ipynb",
     "timestamp": 1640981109479
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
